{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplos programación RDDs Spark Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/maes/.sdkman/candidates/java/current'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# En nuestro ordenador personal, si no esta definida la variable JAVA_HOME, deberemos indicarla\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "\n",
    "# En los laboratorios docentes, sera necesario utilizar la siguiente\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/\"\n",
    "\n",
    "os.environ[\"JAVA_HOME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries, check versions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.3\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openjdk version \"21.0.4\" 2024-07-16 LTS\n",
      "OpenJDK Runtime Environment Temurin-21.0.4+7 (build 21.0.4+7-LTS)\n",
      "OpenJDK 64-Bit Server VM Temurin-21.0.4+7 (build 21.0.4+7-LTS, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/18 01:17:30 WARN Utils: Your hostname, maes-GE72-7RE resolves to a loopback address: 127.0.1.1; using 192.168.1.58 instead (on interface enp3s0)\n",
      "24/11/18 01:17:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/18 01:17:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/18 01:17:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.58:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>101 Spark DataFrames</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7edbe081e7d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(\"local[*]\")\n",
    "         #.config(\"spark.driver.cores\", 1)\n",
    "         .appName(\"101 Spark DataFrames\")\n",
    "         .getOrCreate() )\n",
    "\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones de bajo nivel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([\"#epicfail\",  \"#hadoop\", \"#rstats\",  \"#rstudio\", \"#rstats\", \"#spark\", \"#hadoop\", \"#hdfs\",\n",
    "\"#hadoop\",  \"#oreilly\", \"#spark\", \"#python\", \"#spark\", \"#scala\", \"#spark\", \"#strataconf\", \"#strataconf\", \"#oreilly\",\n",
    "\"#spark\", \"#databricks\", \"#hadoop\", \"#hdfs\", \"#spark\",  \"#hdfs\"], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) PythonRDD[5] at RDD at PythonRDD.scala:53 []\n",
      " |  MapPartitionsRDD[4] at mapPartitions at PythonRDD.scala:160 []\n",
      " |  ShuffledRDD[3] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(4) PairwiseRDD[2] at distinct at /tmp/ipykernel_71389/813733018.py:1 []\n",
      "    |  PythonRDD[1] at distinct at /tmp/ipykernel_71389/813733018.py:1 []\n",
      "    |  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289 []\n"
     ]
    }
   ],
   "source": [
    "rdd_distinct = rdd.distinct()\n",
    "print(rdd_distinct.toDebugString().decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[5] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "print(rdd_distinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 4) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#hdfs', '#oreilly', '#databricks', '#rstudio', '#spark', '#scala', '#strataconf', '#epicfail', '#hadoop', '#rstats', '#python']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(rdd_distinct.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contador de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"data/wordcount_data.txt\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word count from Wikipedia the free encyclopedia',\n",
       " 'the word count is the number of words in a document or passage of text Word counting may be needed when a text',\n",
       " 'is required to stay within certain numbers of words This may particularly be the case in academia legal',\n",
       " 'proceedings journalism and advertising Word count is commonly used by translators to determine the price for',\n",
       " 'the translation job Word counts may also be used to calculate measures of readability and to measure typing']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_word_count =(rdd\n",
    ".flatMap(lambda line: line.split())\n",
    ".map(lambda word: (word, 1))\n",
    ".reduceByKey(lambda x, y: x + y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) PythonRDD[14] at RDD at PythonRDD.scala:53 []\n",
      " |  MapPartitionsRDD[13] at mapPartitions at PythonRDD.scala:160 []\n",
      " |  ShuffledRDD[12] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(4) PairwiseRDD[11] at reduceByKey at /tmp/ipykernel_71389/3516608846.py:4 []\n",
      "    |  PythonRDD[10] at reduceByKey at /tmp/ipykernel_71389/3516608846.py:4 []\n",
      "    |  data/wordcount_data.txt MapPartitionsRDD[7] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "    |  data/wordcount_data.txt HadoopRDD[6] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "print(rdd_word_count.toDebugString().decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('free', 1),\n",
       " ('of', 25),\n",
       " ('in', 11),\n",
       " ('counting', 6),\n",
       " ('may', 8),\n",
       " ('when', 2),\n",
       " ('numbers', 1),\n",
       " ('particularly', 1),\n",
       " ('used', 4),\n",
       " ('job', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_word_count.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_word_count.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_word_count.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = sc.textFile(\"data/wordcount_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r1 -n1\n",
    "rdd_word_count =(rdd2\n",
    ".flatMap(lambda line: line.split())\n",
    ".map(lambda word: (word, 1))\n",
    ".reduceByKey(lambda x, y: x + y)\n",
    ".take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio WordCount - Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio realizaras un conteo de palabras del texto de una colección de Tweets situados en `data/15m-sample.json`.\n",
    "- Los tweets tienen un campo `text` con el texto del tweet\n",
    "- Es posible que necesites la librería `json` para leer correctante un tweet\n",
    "- Deberán devolverse las 5 palabras más repetidas (buscar la operación más adecuada en la [documentación](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
